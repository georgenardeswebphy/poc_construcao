{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71c997-3dcf-4ad6-9852-402bc4b5b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03e2ef-f449-48be-a9c4-63b2b92b96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"tfrecords_position\"\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "      \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "      \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "      \"category_id\": tf.io.FixedLenFeature([], tf.int64),        \n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)    \n",
    "    example[\"image\"] = tf.io.decode_png(example[\"image\"], channels=3)       \n",
    "    return example\n",
    "\n",
    "\n",
    "# cria arquivos de treinamento\n",
    "def prepare_sample_train(features):\n",
    "    rand = tf.random.uniform([5])\n",
    "    noise = tf.random.normal(shape=(224,224,3), mean=0.0, stddev=8, dtype=tf.float32)    \n",
    "    image = tf.cast(features[\"image\"], dtype=tf.float32)\n",
    "    image = tf.image.resize(image, size=(224, 224))   \n",
    "    \n",
    "    #format(x, 'b').zfill(n)\n",
    "    augment_op = format(np.random.randint(0, 31), 'b').zfill(5)\n",
    "    \n",
    "    # BRIGHTNESS\n",
    "    if augment_op[0] == '1':\n",
    "        aux_image = tf.image.random_brightness(image, 0.5)    \n",
    "        image = ((1-rand[1])*image + rand[1]*aux_image)    \n",
    " \n",
    "    # HUE\n",
    "    if augment_op[1] == '1':\n",
    "        aux_image = tf.image.random_hue(image, 0.08)\n",
    "        image = ((1-rand[2])*image + rand[2]*aux_image)\n",
    "    \n",
    "    # CONTRAST\n",
    "    if augment_op[2] == '1':\n",
    "        aux_image = tf.clip_by_value(tf.image.random_contrast(image, 0.8, 1.5), 0, 255)\n",
    "        image = ((1-rand[3])*image + rand[3]*aux_image)\n",
    "\n",
    "    # CROP\n",
    "    if augment_op[3] == '1':\n",
    "        image = tf.image.random_crop(image, (200,200,3))\n",
    "        image = tf.image.resize(image, size=(224, 224))\n",
    "        \n",
    "    # NOISE    \n",
    "    if augment_op[4] == '1':\n",
    "        aux_image = image + (noise * rand[4])\n",
    "        image = tf.clip_by_value(aux_image, 0.0, 255)\n",
    "    \n",
    "    \n",
    "    return image, features[\"category_id\"]\n",
    "\n",
    "# cria arquivos de valid\n",
    "def prepare_sample_valid(features):\n",
    "    image = tf.cast(features[\"image\"], dtype=tf.float32)  \n",
    "    image = tf.image.resize(image, size=(224, 224))   \n",
    "    return image, features[\"category_id\"]\n",
    "\n",
    "\n",
    "def get_train_dataset(filenames, batch_size):\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n",
    "        .map(prepare_sample_train, num_parallel_calls=AUTOTUNE)\n",
    "        .shuffle(batch_size * 10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def get_valid_dataset(filenames, batch_size):\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n",
    "        .map(prepare_sample_valid, num_parallel_calls=AUTOTUNE)\n",
    "        .shuffle(batch_size * 10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68866f2-40eb-43dc-821f-e6cb06f88b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='../dataset_varejo/oclusao/',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(50, 50), seed=10, validation_split=0.03, subset=\"training\")\n",
    "\n",
    "valid_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='../dataset_varejo/oclusao/',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(50, 50), seed=10, validation_split=0.03, subset=\"validation\")\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_filenames = tf.io.gfile.glob(f\"tfrecords_position/train/*.tfrec\")\n",
    "\n",
    "train_ds = get_train_dataset(train_filenames, batch_size)\n",
    "\n",
    "for data in train_ds.take(3).as_numpy_iterator():\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,4))\n",
    "\n",
    "    ax1.imshow(data[0][0] /255.)\n",
    "    ax2.imshow(data[0][1] /255.)\n",
    "    ax3.imshow(data[0][2] /255.)\n",
    "   \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "val_filenames = tf.io.gfile.glob(f\"tfrecords_position/val/*.tfrec\")\n",
    "\n",
    "valid_ds = get_valid_dataset(val_filenames, batch_size)\n",
    "\n",
    "for data in valid_ds.take(3).as_numpy_iterator():\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,4))\n",
    "\n",
    "    ax1.imshow(data[0][0] /255.)\n",
    "    ax2.imshow(data[0][1] /255.)\n",
    "    ax3.imshow(data[0][2] /255.)\n",
    "   \n",
    "    plt.show()    \n",
    "    \n",
    "\"\"\"\n",
    "def is_test(x, y):\n",
    "    return x % 15 == 0\n",
    "\n",
    "def is_train(x, y):\n",
    "    return not is_test(x, y)\n",
    "\n",
    "recover = lambda x,y: y\n",
    "\n",
    "valid_ds = full_ds.enumerate().filter(is_test).map(recover)\n",
    "train_ds = full_ds.enumerate().filter(is_train).map(recover)    \n",
    "\"\"\"\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e8104-aff7-46b6-9a69-2be49fbc32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pre-trained model as backend\n",
    "base_model = keras.applications.MobileNetV3Small(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797fac2-fba8-4c98-b07d-1b5b962bf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('multiply_6').output)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = model.output\n",
    "x = keras.layers.Conv2D(64, (3,3), padding='valid', kernel_regularizer=keras.regularizers.l2(0.0005))(x)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0005))(x)\n",
    "# and a logistic layer \n",
    "predictions = keras.layers.Dense(5, activation='softmax', name=\"output_layer\")(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33daf0db-4938-4392-8a55-9f5bb9a91923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                  loss = 'sparse_categorical_crossentropy',\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "\n",
    "#loss='CategoricalCrossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(train_ds, epochs=50, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ff41c-3d3e-4c07-b470-14a26cdd720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efc17c-4d7d-4833-968b-1e20f66a0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = valid_ds.take(3)\n",
    "\n",
    "\n",
    "for batch in samples.as_numpy_iterator():\n",
    "    images, labels = batch        \n",
    "    \n",
    "    for img, lbl in zip(images[:], labels[:]):\n",
    "        #print(img.shape, lbl)\n",
    "        \n",
    "        o = model(np.expand_dims(img, 0))\n",
    "        if tf.argmax(o, -1).numpy()[0] != lbl:            \n",
    "            print(tf.argmax(o, -1).numpy()[0], lbl)\n",
    "            plt.figure(figsize=(7, 7))\n",
    "            plt.imshow(img/255.)\n",
    "            plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086218f-8100-46fd-812e-de28f8df1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelos/modelotf/\")\n",
    "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names='output_layer' --saved_model_tags=serve \"modelos/modelotf\"  \"modelos/web_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b09a88-8d2b-4352-9b0f-c42876cd1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "layer_name = \"multiply\"\n",
    "extractor = keras.Model(inputs=model.inputs,outputs=model.get_layer(layer_name).output)\n",
    "features = extractor(data[0][0:1])[0,...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9676376-7443-4b92-964e-1274be506e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.numpy().shape)\n",
    "print(features/np.max(features))\n",
    "\n",
    "plt.imshow(features/np.max(features))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb93f29-047d-4656-af66-6ff9ee366d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_ds.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af80a9-9d4e-4741-865f-78597a4d443c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
